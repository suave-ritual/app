{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak9eMerZJHBU",
        "outputId": "93ccb0db-b843-4223-939c-e272c9cf5442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.11/dist-packages (0.2.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.46.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap) (0.61.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap) (0.44.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-5a1ef713cd00>:197: UserWarning: The palette list has more values (4) than needed (3), which may not be intended.\n",
            "  sns.violinplot(data=df, x=group_column, y=y_column, palette=colors, linewidth=LINE_WIDTH)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insights saved to: /content/drive/MyDrive/output_anxiety_latent_causal_graph/insights.txt\n",
            "Execution completed successfully - LLM Interpretation Enhanced Notebook (Refactored with Google Drive).\n"
          ]
        }
      ],
      "source": [
        "!pip install kaleido networkx numpy pandas plotly seaborn shap scikit-learn matplotlib kaleido typing-extensions\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Enhanced Anxiety Intervention Explainability with Fine-Tuned LLM (Google Drive Integration)\n",
        "\n",
        "This notebook simulates fine-tuning a pre-trained language model to generate\n",
        "nuanced and context-aware interpretations of visualizations and statistical\n",
        "analyses, enhancing the explainability of anxiety intervention results.  It\n",
        "saves all outputs to a specified Google Drive folder.\n",
        "\n",
        "Workflow:\n",
        "1. Mount Google Drive: Connects to your Google Drive.\n",
        "2. Data Loading and Validation: Load and validate synthetic anxiety data.\n",
        "3. Data Preprocessing: One-hot encode groups and scale numerical features.\n",
        "4. SHAP Value Analysis: Quantify feature importance.\n",
        "5. Data Visualization: Generate KDE, Violin, Parallel Coordinates, and Hypergraph plots.\n",
        "6. Statistical Summary: Perform bootstrap analysis and generate summary statistics.\n",
        "7. Fine-Tuned LLM Insights Report: Synthesize findings using simulated LLMs.\n",
        "\n",
        "Keywords: Fine-Tuning, Transformers, LLM Interpretation, Explainability, Anxiety Intervention, SHAP, Data Visualization, Google Drive\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "from io import StringIO\n",
        "from typing import List, Dict, Tuple, Callable, Optional\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import shap\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.stats import bootstrap\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Google Drive integration\n",
        "from google.colab import drive\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"plotly\")\n",
        "\n",
        "\n",
        "# --- Constants ---\n",
        "#  This is now relative to the mounted drive.\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/output_anxiety_latent_causal_graph/\"\n",
        "PARTICIPANT_ID_COLUMN = \"participant_id\"\n",
        "GROUP_COLUMN = \"group\"\n",
        "ANXIETY_PRE_COLUMN = \"anxiety_pre\"\n",
        "ANXIETY_POST_COLUMN = \"anxiety_post\"\n",
        "MODEL_GROK_NAME = \"grok-base\"\n",
        "MODEL_CLAUDE_NAME = \"claude-3.7-sonnet\"\n",
        "MODEL_GROK_ENHANCED_NAME = \"grok-enhanced\"  # Simulated fine-tuned model\n",
        "LINE_WIDTH = 2.5\n",
        "NEON_COLORS = [\"#FF00FF\", \"#00FFFF\", \"#FFFF00\", \"#00FF00\"]\n",
        "BOOTSTRAP_RESAMPLES = 500\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def create_output_directory(path: str) -> None:\n",
        "    \"\"\"Creates the output directory on Google Drive if it doesn't exist.\"\"\"\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "def load_data_from_synthetic_string(csv_string: str) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Loads data from a synthetic CSV string.\"\"\"\n",
        "    if not csv_string.strip():\n",
        "        raise ValueError(\"The input CSV string is empty.\")\n",
        "    return pd.read_csv(StringIO(csv_string))\n",
        "\n",
        "\n",
        "def validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> None:\n",
        "    \"\"\"Validates the DataFrame: required columns, data types, IDs, groups, and ranges.\"\"\"\n",
        "    if df is None:\n",
        "        raise ValueError(\"DataFrame is None.\")\n",
        "\n",
        "    missing_columns = set(required_columns) - set(df.columns)\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing columns: {missing_columns}\")\n",
        "\n",
        "    for col in required_columns:\n",
        "        if col not in (PARTICIPANT_ID_COLUMN, GROUP_COLUMN):\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                raise TypeError(f\"Non-numeric values in column: {col}\")\n",
        "\n",
        "    if df[PARTICIPANT_ID_COLUMN].duplicated().any():\n",
        "        raise ValueError(\"Duplicate participant IDs found.\")\n",
        "\n",
        "    valid_groups = [\"Group A\", \"Group B\", \"Control\"]\n",
        "    invalid_groups = df[~df[GROUP_COLUMN].isin(valid_groups)][GROUP_COLUMN].unique()\n",
        "    if invalid_groups.size > 0:\n",
        "        raise ValueError(f\"Invalid group labels: {invalid_groups}\")\n",
        "\n",
        "    for col in (ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN):\n",
        "        if not (0 <= df[col].min() <= 10 and 0 <= df[col].max() <= 10):\n",
        "            raise ValueError(f\"Anxiety scores in '{col}' out of range (0-10).\")\n",
        "\n",
        "\n",
        "def analyze_text_with_llm(text: str, model_name: str) -> str:\n",
        "    \"\"\"Simulates text analysis with different LLMs.\"\"\"\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    if model_name == MODEL_GROK_NAME:\n",
        "        if \"causal graph\" in text_lower:\n",
        "            return \"Grok-base: Causal graph shows basic relationships.\"\n",
        "        if \"shap summary\" in text_lower:\n",
        "            return \"Grok-base: SHAP values indicate feature importance generally.\"\n",
        "        return f\"Grok-base: Initial analysis on '{text}'.\"\n",
        "\n",
        "    if model_name == MODEL_CLAUDE_NAME:\n",
        "        if \"kde plot\" in text_lower:\n",
        "            return \"Claude 3.7: KDE plot visually compares anxiety distributions, revealing overlaps and separations.\"\n",
        "        if \"violin plot\" in text_lower:\n",
        "            return \"Claude 3.7: Violin plot details distribution shapes, highlighting group-specific variations.\"\n",
        "        return f\"Claude 3.7: Enhanced visual analysis on '{text}'.\"\n",
        "\n",
        "    if model_name == MODEL_GROK_ENHANCED_NAME:\n",
        "        if \"shap summary\" in text_lower:\n",
        "            return \"Grok-Enhanced (Fine-Tuned): SHAP summary reveals pre-anxiety as dominant, but group membership also contributes, suggesting a moderated effect.\"\n",
        "        if \"parallel coordinates\" in text_lower:\n",
        "            return \"Grok-Enhanced (Fine-Tuned): Parallel coordinates show individual trajectories and group patterns, with clear anxiety reduction but variability.\"\n",
        "        if \"causal graph\" in text_lower:\n",
        "            return \"Grok-Enhanced (Fine-Tuned): Causal graph suggests a moderated pathway; group membership influences the impact of pre-anxiety.\"\n",
        "        if \"kde plot\" in text_lower:\n",
        "            return \"Grok-Enhanced (Fine-Tuned): KDE plot shows a shift towards lower anxiety, more pronounced in Group A.\"\n",
        "        if \"violin plot\" in text_lower:\n",
        "            return \"Grok-Enhanced (Fine-Tuned): Violin plot shows decreased median anxiety, but differing distributions suggest varying responsiveness.\"\n",
        "        if \"hypergraph\" in text_lower:\n",
        "            return \"Grok-Enhanced (Fine-Tuned): Hypergraph highlights clusters with similar profiles, suggesting subgroups and personalized strategies.\"\n",
        "        return f\"Grok-Enhanced (Fine-Tuned): Context-aware analysis on '{text}'. Provides nuanced, actionable insights.\"\n",
        "\n",
        "    raise ValueError(f\"Model '{model_name}' not supported.\")\n",
        "\n",
        "\n",
        "def scale_data(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"Scales specified columns using MinMaxScaler.\"\"\"\n",
        "    if df.empty:\n",
        "        raise ValueError(\"Input DataFrame is empty.\")\n",
        "    for col in columns:\n",
        "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "            raise ValueError(f\"Column '{col}' is not numeric.\")\n",
        "    scaler = MinMaxScaler()\n",
        "    df[columns] = scaler.fit_transform(df[columns])\n",
        "    return df\n",
        "\n",
        "\n",
        "def calculate_shap_values(df: pd.DataFrame, feature_columns: List[str], target_column: str, output_path: str) -> str:\n",
        "    \"\"\"Calculates and plots SHAP values.\"\"\"\n",
        "    if not all(col in df.columns for col in feature_columns):\n",
        "        raise ValueError(\"Feature columns not found in DataFrame.\")\n",
        "    if target_column not in df.columns or not pd.api.types.is_numeric_dtype(df[target_column]):\n",
        "        raise ValueError(\"Target column issue.\")\n",
        "\n",
        "    model_rf = RandomForestRegressor(random_state=42)\n",
        "    model_rf.fit(df[feature_columns], df[target_column])\n",
        "    explainer = shap.TreeExplainer(model_rf)\n",
        "    shap_values = explainer.shap_values(df[feature_columns])\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.style.use('dark_background')\n",
        "    shap.summary_plot(shap_values, df[feature_columns], show=False, color_bar=True)\n",
        "    plt.savefig(os.path.join(output_path, 'shap_summary.png'))\n",
        "    plt.close()\n",
        "    return f\"SHAP summary for features {feature_columns} predicting {target_column}\"\n",
        "\n",
        "\n",
        "def create_kde_plot(df: pd.DataFrame, column1: str, column2: str, output_path: str, colors: List[str]) -> str:\n",
        "    \"\"\"Creates a KDE plot comparing two columns.\"\"\"\n",
        "    for col in (column1, column2):\n",
        "        if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):\n",
        "            raise ValueError(f\"Invalid column '{col}' for KDE plot.\")\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.style.use('dark_background')\n",
        "    sns.kdeplot(data=df[column1], color=colors[0], label=column1.capitalize(), linewidth=LINE_WIDTH)\n",
        "    sns.kdeplot(data=df[column2], color=colors[1], label=column2.capitalize(), linewidth=LINE_WIDTH)\n",
        "    plt.title('KDE Plot of Anxiety Levels', color='white')\n",
        "    plt.legend(facecolor='black', edgecolor='white', labelcolor='white')\n",
        "    plt.savefig(os.path.join(output_path, 'kde_plot.png'))\n",
        "    plt.close()\n",
        "    return f\"KDE plot visualizing distributions of {column1} and {column2}\"\n",
        "\n",
        "\n",
        "def create_violin_plot(df: pd.DataFrame, group_column: str, y_column: str, output_path: str, colors: List[str]) -> str:\n",
        "    \"\"\"Creates a violin plot.\"\"\"\n",
        "    if group_column not in df.columns:\n",
        "        raise ValueError(f\"Group column '{group_column}' not found.\")\n",
        "    if y_column not in df.columns or not pd.api.types.is_numeric_dtype(df[y_column]):\n",
        "        raise ValueError(f\"Invalid Y column '{y_column}' for violin plot.\")\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.style.use('dark_background')\n",
        "    sns.violinplot(data=df, x=group_column, y=y_column, palette=colors, linewidth=LINE_WIDTH)\n",
        "    plt.title('Violin Plot of Anxiety Distribution by Group', color='white')\n",
        "    plt.savefig(os.path.join(output_path, 'violin_plot.png'))\n",
        "    plt.close()\n",
        "    return f\"Violin plot showing {y_column} across {group_column}\"\n",
        "\n",
        "\n",
        "def create_parallel_coordinates_plot(df: pd.DataFrame, group_column: str, anxiety_pre_column: str, anxiety_post_column: str, output_path: str, colors: List[str]) -> str:\n",
        "    \"\"\"Creates a parallel coordinates plot.\"\"\"\n",
        "    required_cols = (group_column, anxiety_pre_column, anxiety_post_column)\n",
        "    if not all(col in df.columns for col in required_cols) or not all(pd.api.types.is_numeric_dtype(df[col]) for col in required_cols[1:]):\n",
        "        raise ValueError(\"Invalid columns for parallel coordinates plot.\")\n",
        "\n",
        "    plot_df = df[list(required_cols)].copy()\n",
        "    plot_df = pd.get_dummies(plot_df, columns=[group_column], prefix=group_column)\n",
        "    encoded_group_cols = [col for col in plot_df.columns if col.startswith(f\"{group_column}_\")]\n",
        "\n",
        "    def get_group_color(row):\n",
        "        for i, col in enumerate(encoded_group_cols):\n",
        "            if row[col] == 1:\n",
        "                return colors[i % len(colors)]\n",
        "        return 'gray'\n",
        "\n",
        "    plot_df['color'] = plot_df[encoded_group_cols].apply(get_group_color, axis=1)\n",
        "\n",
        "    fig = px.parallel_coordinates(plot_df, color='color', dimensions=[anxiety_pre_column, anxiety_post_column],\n",
        "                                  title=\"Anxiety Levels: Pre- vs Post-Intervention by Group\")\n",
        "    fig.update_layout(plot_bgcolor='black', paper_bgcolor='black', font_color='white', title_font_size=16)\n",
        "    # Use plotly's io module to save the image, handling potential kaleido issues\n",
        "    try:\n",
        "        fig.write_image(os.path.join(output_path, 'parallel_coordinates_plot.png'))\n",
        "    except ValueError as e:\n",
        "        if \"kaleido\" in str(e).lower():\n",
        "            print(\"Error: Kaleido is required for image export. Please install it using 'pip install kaleido'.\")\n",
        "            print(\"Skipping saving the parallel coordinates plot.\")\n",
        "        else:  # Re-raise if it's a different ValueError\n",
        "            raise\n",
        "    return \"Parallel coordinates plot of anxiety pre vs post intervention by group\"\n",
        "\n",
        "\n",
        "def visualize_hypergraph(df: pd.DataFrame, anxiety_pre_column: str, anxiety_post_column: str, output_path: str, colors: List[str]) -> str:\n",
        "    \"\"\"Visualizes relationships using a hypergraph.\"\"\"\n",
        "    required_cols = (PARTICIPANT_ID_COLUMN, anxiety_pre_column, anxiety_post_column)\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        raise ValueError(\"Required columns not found for hypergraph.\")\n",
        "\n",
        "    G = nx.Graph()\n",
        "    participant_ids = df[PARTICIPANT_ID_COLUMN].tolist()\n",
        "    G.add_nodes_from(participant_ids, bipartite=0)\n",
        "    feature_sets = {\n",
        "        \"anxiety_pre\": df[PARTICIPANT_ID_COLUMN][df[anxiety_pre_column] > df[anxiety_pre_column].mean()].tolist(),\n",
        "        \"anxiety_post\": df[PARTICIPANT_ID_COLUMN][df[anxiety_post_column] > df[anxiety_post_column].mean()].tolist()\n",
        "    }\n",
        "    feature_nodes = list(feature_sets.keys())\n",
        "    G.add_nodes_from(feature_nodes, bipartite=1)\n",
        "    for feature, participants in feature_sets.items():\n",
        "        for participant in participants:\n",
        "            G.add_edge(participant, feature)\n",
        "    pos = nx.bipartite_layout(G, participant_ids)\n",
        "    color_map = [colors[0] if node in participant_ids else colors[1] for node in G]\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.style.use('dark_background')\n",
        "    nx.draw(G, pos, with_labels=True, node_color=color_map, font_color=\"white\", edge_color=\"gray\", width=LINE_WIDTH, node_size=700, font_size=10)\n",
        "    plt.title(\"Hypergraph Representation of Anxiety Patterns\", color=\"white\")\n",
        "    plt.savefig(os.path.join(output_path, \"hypergraph.png\"))\n",
        "    plt.close()\n",
        "    return \"Hypergraph visualizing participant relationships.\"\n",
        "\n",
        "\n",
        "def perform_bootstrap(data: pd.Series, statistic: Callable, n_resamples: int = BOOTSTRAP_RESAMPLES) -> Tuple[Optional[float], Optional[float]]:\n",
        "    \"\"\"Performs bootstrap resampling and returns the confidence interval.\"\"\"\n",
        "    if data.empty or not pd.api.types.is_numeric_dtype(data):\n",
        "        raise ValueError(\"Invalid input data for bootstrap.\")\n",
        "\n",
        "    bootstrap_result = bootstrap((data,), statistic, n_resamples=n_resamples, method='percentile', random_state=42)\n",
        "    return bootstrap_result.confidence_interval.low, bootstrap_result.confidence_interval.high\n",
        "\n",
        "\n",
        "def save_summary(df: pd.DataFrame, bootstrap_ci: Tuple[Optional[float], Optional[float]], output_path: str) -> str:\n",
        "    \"\"\"Calculates and saves summary statistics.\"\"\"\n",
        "    ci_string = f\"[{bootstrap_ci[0]:.4f}, {bootstrap_ci[1]:.4f}]\" if all(x is not None for x in bootstrap_ci) else \"Could not calculate Bootstrap CI\"\n",
        "    summary_stats_text = df.describe().to_string() + f\"\\nBootstrap CI for anxiety_post mean: {ci_string}\"\n",
        "    with open(os.path.join(output_path, 'summary.txt'), 'w') as f:\n",
        "        f.write(summary_stats_text)\n",
        "    return summary_stats_text\n",
        "\n",
        "\n",
        "def generate_insights_report(summary_stats_text: str, shap_analysis_info: str, kde_plot_desc: str, violin_plot_desc: str, parallel_coords_desc: str, hypergraph_desc: str, output_path: str) -> None:\n",
        "    \"\"\"Generates a comprehensive insights report using simulated LLMs.\"\"\"\n",
        "    grok_insights = (\n",
        "        analyze_text_with_llm(f\"Analyze summary statistics:\\n{summary_stats_text}\", MODEL_GROK_NAME) + \"\\n\\n\" +\n",
        "        analyze_text_with_llm(f\"Explain SHAP summary: {shap_analysis_info}\", MODEL_GROK_NAME)\n",
        "    )\n",
        "    claude_insights = (\n",
        "        analyze_text_with_llm(f\"Interpret KDE plot: {kde_plot_desc}\", MODEL_CLAUDE_NAME) + \"\\n\\n\" +\n",
        "        analyze_text_with_llm(f\"Interpret Violin plot: {violin_plot_desc}\", MODEL_CLAUDE_NAME) + \"\\n\\n\" +\n",
        "        analyze_text_with_llm(f\"Interpret Parallel Coordinates Plot: {parallel_coords_desc}\", MODEL_CLAUDE_NAME) + \"\\n\\n\" +\n",
        "        analyze_text_with_llm(f\"Interpret Hypergraph: {hypergraph_desc}\", MODEL_CLAUDE_NAME)\n",
        "    )\n",
        "    grok_enhanced_insights = analyze_text_with_llm(\"Provide enhanced, context-aware insights on anxiety intervention effectiveness, integrating all analyses.\", MODEL_GROK_ENHANCED_NAME)\n",
        "\n",
        "    combined_insights = f\"\"\"\n",
        "Combined Insights Report: Anxiety Intervention Analysis\n",
        "\n",
        "Grok-base Analysis:\n",
        "{grok_insights}\n",
        "\n",
        "Claude 3.7 Sonnet Analysis:\n",
        "{claude_insights}\n",
        "\n",
        "Grok-Enhanced Analysis (Fine-Tuned):\n",
        "{grok_enhanced_insights}\n",
        "\n",
        "Synthesized Summary:\n",
        "This report presents a synthesized analysis of anxiety intervention effectiveness, leveraging a Mixture of Experts approach with simulated fine-tuning of the Grok-Enhanced LLM.  Grok-base provides initial interpretations. Claude 3.7 offers enhanced visual analysis. The Grok-Enhanced model, simulating fine-tuning, provides significantly more nuanced and context-aware interpretations. It identifies pre-anxiety as the dominant predictor but acknowledges the moderating role of group membership. The combined analyses provide a robust and explainable understanding of the intervention's effects.\n",
        "\"\"\"\n",
        "    with open(os.path.join(output_path, 'insights.txt'), 'w') as f:\n",
        "        f.write(combined_insights)\n",
        "    print(f\"Insights saved to: {os.path.join(output_path, 'insights.txt')}\")\n",
        "\n",
        "\n",
        "# --- Main Script ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    create_output_directory(OUTPUT_PATH)\n",
        "\n",
        "    # More varied and slightly larger synthetic dataset\n",
        "    synthetic_dataset = \"\"\"\n",
        "participant_id,group,anxiety_pre,anxiety_post\n",
        "P001,Group A,4,2\n",
        "P002,Group A,3,1\n",
        "P003,Group A,5,1\n",
        "P004,Group B,6,5\n",
        "P005,Group B,5,4\n",
        "P006,Group B,7,7\n",
        "P007,Control,3,3\n",
        "P008,Control,4,5\n",
        "P009,Control,2,2\n",
        "P010,Control,5,4\n",
        "P011,Group A,6,3\n",
        "P012,Group A,2,0\n",
        "P013,Group B,8,6\n",
        "P014,Group B,7,5\n",
        "P015,Control,4,4\n",
        "\"\"\"\n",
        "    df = load_data_from_synthetic_string(synthetic_dataset)\n",
        "    required_columns = [PARTICIPANT_ID_COLUMN, GROUP_COLUMN, ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN]\n",
        "    validate_dataframe(df, required_columns)\n",
        "\n",
        "    df_original = df.copy()  # Keep original for visualizations\n",
        "\n",
        "    df = pd.get_dummies(df, columns=[GROUP_COLUMN], prefix=GROUP_COLUMN, drop_first=False)\n",
        "    encoded_group_cols = [col for col in df.columns if col.startswith(f\"{GROUP_COLUMN}_\")]  # Corrected line\n",
        "    df = scale_data(df, [ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN] + encoded_group_cols)\n",
        "\n",
        "    # SHAP analysis\n",
        "    shap_feature_columns = encoded_group_cols + [ANXIETY_PRE_COLUMN]\n",
        "    shap_analysis_info = calculate_shap_values(df, shap_feature_columns, ANXIETY_POST_COLUMN, OUTPUT_PATH)\n",
        "\n",
        "    # Visualizations (using df_original for plots needing original group labels)\n",
        "    kde_plot_desc = create_kde_plot(df, ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN, OUTPUT_PATH, NEON_COLORS[:2])\n",
        "    violin_plot_desc = create_violin_plot(df_original, GROUP_COLUMN, ANXIETY_POST_COLUMN, OUTPUT_PATH, NEON_COLORS)\n",
        "    parallel_coords_desc = create_parallel_coordinates_plot(df_original, GROUP_COLUMN, ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN, OUTPUT_PATH, NEON_COLORS)\n",
        "    hypergraph_desc = visualize_hypergraph(df_original, ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN, OUTPUT_PATH, NEON_COLORS[:2])\n",
        "\n",
        "    # Statistical analysis\n",
        "    bootstrap_ci = perform_bootstrap(df[ANXIETY_POST_COLUMN], np.mean)\n",
        "    summary_stats_text = save_summary(df, bootstrap_ci, OUTPUT_PATH)\n",
        "\n",
        "    # Generate insights report\n",
        "    generate_insights_report(summary_stats_text, shap_analysis_info, kde_plot_desc, violin_plot_desc, parallel_coords_desc, hypergraph_desc, OUTPUT_PATH)\n",
        "\n",
        "    print(\"Execution completed successfully - LLM Interpretation Enhanced Notebook (Refactored with Google Drive).\")"
      ]
    }
  ]
}